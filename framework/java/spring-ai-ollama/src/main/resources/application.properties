# Spring AI Ollama Configuration
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.model=tinyllama

# Server Configuration
server.port=8080

# Thymeleaf Configuration
spring.thymeleaf.cache=false
spring.thymeleaf.prefix=classpath:/templates/
spring.thymeleaf.suffix=.html

# Logging Configuration
logging.level.zama.learning.spring.ai.ollama=DEBUG
logging.level.org.springframework.ai=DEBUG
logging.level.root=INFO

# Management Endpoints
management.endpoints.web.exposure.include=health,info
management.endpoint.health.show-details=when-authorized
