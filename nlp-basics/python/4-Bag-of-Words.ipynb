{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b977f6e",
   "metadata": {},
   "source": [
    "## How Bag of Words Works\n",
    "\n",
    "### Step-by-Step Process:\n",
    "\n",
    "1. **Create Vocabulary**: Extract all unique words from the entire corpus\n",
    "2. **Vectorize Documents**: For each document, count word occurrences\n",
    "3. **Build Feature Matrix**: Each row = document, each column = word from vocabulary\n",
    "4. **Classification**: Use the numerical vectors for machine learning algorithms\n",
    "\n",
    "### Example:\n",
    "```\n",
    "Documents:\n",
    "- \"I love machine learning\"\n",
    "- \"Machine learning is amazing\"\n",
    "- \"I love programming\"\n",
    "\n",
    "Vocabulary: [\"I\", \"love\", \"machine\", \"learning\", \"is\", \"amazing\", \"programming\"]\n",
    "\n",
    "BoW Vectors:\n",
    "Doc1: [1, 1, 1, 1, 0, 0, 0]\n",
    "Doc2: [0, 0, 1, 1, 1, 1, 0]\n",
    "Doc3: [1, 1, 0, 0, 0, 0, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c2bf4a",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of Bag of Words\n",
    "\n",
    "### âœ… Advantages:\n",
    "\n",
    "1. **Simplicity**: Easy to understand and implement\n",
    "2. **Fast Training**: Quick to compute and train models\n",
    "3. **Memory Efficient**: Sparse matrices save memory\n",
    "4. **Baseline Performance**: Good starting point for text classification\n",
    "5. **Interpretability**: Feature weights are easily interpretable\n",
    "6. **Language Agnostic**: Works with any language\n",
    "\n",
    "### âŒ Disadvantages:\n",
    "\n",
    "1. **Lost Context**: Ignores word order and grammar\n",
    "2. **Semantic Gaps**: Cannot capture word relationships\n",
    "3. **Sparse Vectors**: High dimensionality with many zeros\n",
    "4. **Vocabulary Dependency**: Performance depends on vocabulary size\n",
    "5. **No Semantic Similarity**: \"good\" and \"excellent\" are treated as different\n",
    "6. **Common Words Dominance**: Frequent words may overshadow important rare words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc898ef9",
   "metadata": {},
   "source": [
    "## Practical Tips and Variations\n",
    "\n",
    "### ðŸ› ï¸ Optimization Techniques:\n",
    "\n",
    "1. **Text Preprocessing**:\n",
    "   - Remove stop words\n",
    "   - Convert to lowercase\n",
    "   - Handle punctuation\n",
    "   - Stemming/Lemmatization\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - Limit vocabulary size (`max_features`)\n",
    "   - Set minimum document frequency (`min_df`)\n",
    "   - Set maximum document frequency (`max_df`)\n",
    "   - Use n-grams for context\n",
    "\n",
    "3. **Weighting Schemes**:\n",
    "   - **Binary BoW**: 1 if word present, 0 otherwise\n",
    "   - **TF-IDF**: Term Frequency Ã— Inverse Document Frequency\n",
    "   - **Normalized Counts**: Scale by document length\n",
    "\n",
    "### ðŸ”„ Variations:\n",
    "\n",
    "- **N-grams**: Include word sequences (bigrams, trigrams)\n",
    "- **Character-level BoW**: Use character n-grams instead of words\n",
    "- **Weighted BoW**: Apply different weights to different words\n",
    "- **Hash Vectorization**: Use hashing to reduce memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948250bb",
   "metadata": {},
   "source": [
    "## Common Use Cases\n",
    "\n",
    "### ðŸ“Š Applications where BoW works well:\n",
    "\n",
    "1. **Document Classification**: Email spam detection, news categorization\n",
    "2. **Sentiment Analysis**: Basic positive/negative classification\n",
    "3. **Topic Modeling**: Identifying document topics\n",
    "4. **Information Retrieval**: Search engines and document matching\n",
    "5. **Content Filtering**: Content moderation systems\n",
    "6. **Feature Engineering**: As input features for other ML models\n",
    "\n",
    "### ðŸŽ¯ When to use BoW:\n",
    "- Quick prototyping and baseline models\n",
    "- Limited computational resources\n",
    "- Small to medium-sized datasets\n",
    "- When word order is less important\n",
    "- Document-level classification tasks\n",
    "\n",
    "### ðŸš« When NOT to use BoW:\n",
    "- Need semantic understanding\n",
    "- Word order matters (e.g., sentiment in \"not good\")\n",
    "- Working with long sequences\n",
    "- Need contextual embeddings\n",
    "- Complex NLP tasks (translation, QA)\n",
    "\n",
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "1. **BoW is fundamental**: Essential building block in NLP\n",
    "2. **Simple but effective**: Despite limitations, works well for many tasks\n",
    "3. **Good baseline**: Always start with BoW before complex models\n",
    "4. **Preprocessing matters**: Clean text improves performance\n",
    "5. **Combine with other techniques**: Often used with other features\n",
    "6. **Consider alternatives**: Word embeddings for semantic tasks\n",
    "\n",
    "---\n",
    "\n",
    "*Bag of Words remains one of the most important and widely-used techniques in text classification, providing a solid foundation for understanding more advanced NLP methods.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e860fbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Documents:\n",
      "Doc 1: 'I love programming'\n",
      "Doc 2: 'Python is great for programming'\n",
      "Doc 3: 'I love Python'\n",
      "Doc 4: 'Machine learning is great'\n",
      "\n",
      "Vocabulary: ['for', 'great', 'i', 'is', 'learning', 'love', 'machine', 'programming', 'python']\n",
      "Vocabulary size: 9\n",
      "\n",
      "Bag of Words Representation:\n",
      "Vocabulary: ['for', 'great', 'i', 'is', 'learning', 'love', 'machine', 'programming', 'python']\n",
      "--------------------------------------------------\n",
      "Doc 1: [0, 0, 1, 0, 0, 1, 0, 1, 0]\n",
      "      'I love programming'\n",
      "Doc 2: [1, 1, 0, 1, 0, 0, 0, 1, 1]\n",
      "      'Python is great for programming'\n",
      "Doc 3: [0, 0, 1, 0, 0, 1, 0, 0, 1]\n",
      "      'I love Python'\n",
      "Doc 4: [0, 1, 0, 1, 1, 0, 1, 0, 0]\n",
      "      'Machine learning is great'\n",
      "\n",
      "Word-by-word breakdown:\n",
      "\n",
      "Doc 1: 'I love programming'\n",
      "  'i': 1\n",
      "  'love': 1\n",
      "  'programming': 1\n",
      "\n",
      "Doc 2: 'Python is great for programming'\n",
      "  'for': 1\n",
      "  'great': 1\n",
      "  'is': 1\n",
      "  'programming': 1\n",
      "  'python': 1\n",
      "\n",
      "Doc 3: 'I love Python'\n",
      "  'i': 1\n",
      "  'love': 1\n",
      "  'python': 1\n",
      "\n",
      "Doc 4: 'Machine learning is great'\n",
      "  'great': 1\n",
      "  'is': 1\n",
      "  'learning': 1\n",
      "  'machine': 1\n"
     ]
    }
   ],
   "source": [
    "# Simple Bag of Words Example\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love programming\",\n",
    "    \"Python is great for programming\", \n",
    "    \"I love Python\",\n",
    "    \"Machine learning is great\"\n",
    "]\n",
    "\n",
    "print(\"Original Documents:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Doc {i+1}: '{doc}'\")\n",
    "\n",
    "# Step 1: Create vocabulary (all unique words)\n",
    "vocabulary = set()\n",
    "for doc in documents:\n",
    "    words = doc.lower().split()  # Convert to lowercase and split\n",
    "    vocabulary.update(words)\n",
    "\n",
    "vocabulary = sorted(list(vocabulary))  # Sort for consistency\n",
    "print(f\"\\nVocabulary: {vocabulary}\")\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "# Step 2: Create Bag of Words vectors\n",
    "bow_vectors = []\n",
    "\n",
    "for doc in documents:\n",
    "    words = doc.lower().split()\n",
    "    # Count occurrences of each vocabulary word in this document\n",
    "    vector = []\n",
    "    for vocab_word in vocabulary:\n",
    "        count = words.count(vocab_word)\n",
    "        vector.append(count)\n",
    "    bow_vectors.append(vector)\n",
    "\n",
    "# Step 3: Display the results\n",
    "print(f\"\\nBag of Words Representation:\")\n",
    "print(f\"Vocabulary: {vocabulary}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, vector in enumerate(bow_vectors):\n",
    "    print(f\"Doc {i+1}: {vector}\")\n",
    "    print(f\"      '{documents[i]}'\")\n",
    "\n",
    "# Step 4: Show which words contribute to each document\n",
    "print(f\"\\nWord-by-word breakdown:\")\n",
    "for i, vector in enumerate(bow_vectors):\n",
    "    print(f\"\\nDoc {i+1}: '{documents[i]}'\")\n",
    "    for j, count in enumerate(vector):\n",
    "        if count > 0:\n",
    "            print(f\"  '{vocabulary[j]}': {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
